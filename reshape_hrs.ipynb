{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from IPython.display import displayimport os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Stata file: /Users/michaelzhu/Desktop/Stanford/CS229/randhrs1992_2020v2_STATA/randhrs1992_2020v2.dta\n",
      "Converting to pickle: /Users/michaelzhu/Desktop/Stanford/CS229/randhrs1992_2020v2_STATA/randhrs1992_2020v2.pkl\n",
      "Conversion complete. File saved as: /Users/michaelzhu/Desktop/Stanford/CS229/randhrs1992_2020v2_STATA/randhrs1992_2020v2.pkl\n"
     ]
    }
   ],
   "source": [
    "def convert_stata_file(input_path, output_format='csv', output_path=None):\n",
    "    \"\"\"\n",
    "    Convert Stata .dta file to CSV or PKL format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_path : str\n",
    "        Path to the input .dta file\n",
    "    output_format : str\n",
    "        Desired output format ('csv' or 'pkl')\n",
    "    output_path : str, optional\n",
    "        Path for the output file. If None, will use the same name as input\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the converted file\n",
    "    \"\"\"\n",
    "    # Validate input file exists\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "        \n",
    "    # Validate output format\n",
    "    if output_format.lower() not in ['csv', 'pkl']:\n",
    "        raise ValueError(\"Output format must be 'csv' or 'pkl'\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_path is None:\n",
    "        base_path = os.path.splitext(input_path)[0]\n",
    "        output_path = f\"{base_path}.{output_format.lower()}\"\n",
    "    \n",
    "    try:\n",
    "        # Read the Stata file\n",
    "        print(f\"Reading Stata file: {input_path}\")\n",
    "        df = pd.read_stata(input_path)\n",
    "        \n",
    "        # Convert to specified format\n",
    "        if output_format.lower() == 'csv':\n",
    "            print(f\"Converting to CSV: {output_path}\")\n",
    "            df.to_csv(output_path, index=False)\n",
    "        else:\n",
    "            print(f\"Converting to pickle: {output_path}\")\n",
    "            df.to_pickle(output_path)\n",
    "            \n",
    "        print(f\"Conversion complete. File saved as: {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversion: {str(e)}\")\n",
    "        raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_stata_file(\n",
    "    input_path=\"/Users/michaelzhu/Desktop/Stanford/CS229/randhrs1992_2020v2_STATA/randhrs1992_2020v2.dta\",\n",
    "    output_format=\"pkl\",\n",
    "    output_path=\"/Users/michaelzhu/Desktop/Stanford/CS229/randhrs1992_2020v2_STATA/randhrs1992_2020v2.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/Users/michaelzhu/Desktop/Stanford/CS229/randhrs1992_2020v2_STATA/randhrs1992_2020v2.pkl\")\n",
    "print(f\"{len(df)} rows and {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_vars(df: pd.DataFrame, wave: int, prefix: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get variables that match specific patterns for a given wave and prefix.\n",
    "    \"\"\"\n",
    "    selected_vars = ['hhidpn', f'inw{wave}']\n",
    "    \n",
    "    # Get all column names\n",
    "    all_vars = df.columns.tolist()\n",
    "    \n",
    "    # Regular expressions for matching\n",
    "    wave_pattern = f\"{prefix}{wave}[A-Za-z]\"\n",
    "    always_pattern = f\"^{prefix}a[A-Za-z]\"\n",
    "    \n",
    "    # Add matching variables\n",
    "    for var in all_vars:\n",
    "        if re.search(wave_pattern, var) or re.search(always_pattern, var):\n",
    "            selected_vars.append(var)\n",
    "            \n",
    "    return selected_vars\n",
    "\n",
    "def process_wave_data(df: pd.DataFrame, wave: int, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process data for a specific wave and prefix.\n",
    "    \"\"\"\n",
    "    # Get relevant variables\n",
    "    selected_vars = get_selected_vars(df, wave, prefix)\n",
    "    \n",
    "    # Select variables and filter rows\n",
    "    wave_df = df[selected_vars].copy()\n",
    "    wave_df = wave_df[wave_df[f'inw{wave}'] == 1].copy()\n",
    "    \n",
    "    # Add wave column\n",
    "    wave_df['wave'] = wave\n",
    "    \n",
    "    # Rename columns - remove wave number from variable names\n",
    "    rename_dict = {}\n",
    "    for var in selected_vars:\n",
    "        if prefix == 's' and var in [f's{wave}tr20', f's{wave}tr40']:\n",
    "            # Special case for tr20 and tr40\n",
    "            new_name = var.replace(f's{wave}tr', f's{wave}rtr')\n",
    "            rename_dict[var] = new_name\n",
    "            \n",
    "        if var != 'hhidpn' and var != f'inw{wave}':\n",
    "            new_name = var.replace(f'{prefix}{wave}', prefix)\n",
    "            rename_dict[var] = new_name\n",
    "    \n",
    "    wave_df = wave_df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Drop inw column\n",
    "    if prefix in ['r', 'h']:\n",
    "        wave_df = wave_df.drop(columns=[f'inw{wave}'])\n",
    "        \n",
    "    return wave_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/8tg6b2p95rs0b9f1frtgrxvh0000gn/T/ipykernel_33408/1822511503.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_data[prefix] = pd.concat(prefix_data, axis=0, ignore_index=True)\n",
      "/var/folders/f8/8tg6b2p95rs0b9f1frtgrxvh0000gn/T/ipykernel_33408/1822511503.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_data[prefix] = pd.concat(prefix_data, axis=0, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def get_selected_vars(data_dict, wave_num, prefix):\n",
    "    \"\"\"\n",
    "    Get selected variables based on wave number and prefix pattern\n",
    "    Similar to Stata's describe and regex matching functionality\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary or similar containing variable information\n",
    "        wave_num: Wave number (1-15)\n",
    "        prefix: Prefix to match ('s', 'r', or 'h')\n",
    "    \"\"\"\n",
    "    selected_vars = ['hhidpn', f'inw{wave_num}']\n",
    "    \n",
    "    for var in data_dict.columns:\n",
    "        # Match wave-specific variables\n",
    "        if re.match(f'{prefix}{wave_num}[A-Za-z]', var):\n",
    "            selected_vars.append(var)\n",
    "        # Match common variables across waves\n",
    "        if re.match(f'^{prefix}a[A-Za-z]', var):\n",
    "            selected_vars.append(var)\n",
    "            \n",
    "    return selected_vars\n",
    "\n",
    "def process_wave_data(data, wave_num, prefix):\n",
    "    \"\"\"\n",
    "    Process data for a specific wave and prefix\n",
    "    \n",
    "    Args:\n",
    "        data: pandas DataFrame containing the HRS data\n",
    "        wave_num: Wave number (1-15)\n",
    "        prefix: Prefix to match ('s', 'r', or 'h')\n",
    "    \"\"\"\n",
    "    # Get selected variables\n",
    "    selected_vars = get_selected_vars(data, wave_num, prefix)\n",
    "    \n",
    "    # Subset data\n",
    "    wave_data = data[selected_vars].copy()\n",
    "    wave_data = wave_data[wave_data[f'inw{wave_num}'] != 0].copy()\n",
    "    \n",
    "    # Add wave indicator\n",
    "    wave_data['wave'] = wave_num\n",
    "    \n",
    "    # Special handling for 's' prefix\n",
    "    if prefix == 's':\n",
    "        # Handle special cases for tr20 and tr40\n",
    "        selected_vars = [v.replace('tr20', 'rtr20').replace('tr40', 'rtr40') \n",
    "                        for v in selected_vars]\n",
    "    \n",
    "    # Rename variables to remove wave numbers\n",
    "    rename_dict = {old: new.replace(f'{prefix}{wave_num}', prefix) \n",
    "                  for old, new in zip(selected_vars, selected_vars)}\n",
    "    wave_data = wave_data.rename(columns=rename_dict)\n",
    "    \n",
    "    # Drop inw column if not 's' prefix\n",
    "    if prefix != 's':\n",
    "        wave_data = wave_data.drop(columns=[f'inw{wave_num}'])\n",
    "        \n",
    "    return wave_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original data\n",
    "hrs_data = df\n",
    "\n",
    "# Process each prefix type\n",
    "prefixes = ['s', 'r', 'h']\n",
    "combined_data = {}\n",
    "\n",
    "for prefix in prefixes:\n",
    "    # Process each wave\n",
    "    prefix_data = []\n",
    "    for wave in range(1, 16):  # 1 to 15\n",
    "        wave_data = process_wave_data(hrs_data, wave, prefix)\n",
    "        prefix_data.append(wave_data)\n",
    "    \n",
    "    # Combine all waves for this prefix\n",
    "    combined_data[prefix] = pd.concat(prefix_data, axis=0, ignore_index=True)\n",
    "\n",
    "# Merge all prefix datasets\n",
    "final_data = combined_data['s']\n",
    "for prefix in ['r', 'h']:\n",
    "    final_data = pd.merge(\n",
    "        final_data, \n",
    "        combined_data[prefix],\n",
    "        on=['hhidpn', 'wave'],\n",
    "        how='inner',\n",
    "        validate='1:1'\n",
    "    )\n",
    "\n",
    "# Save the final dataset\n",
    "final_data.to_pickle('randhrs1992_2020v2_long_compact.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
